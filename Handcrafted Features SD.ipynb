{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "#for dictionary creation\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, json\n",
    "import ast\n",
    "#feature extraction\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize,pos_tag_sents,WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "#classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC \n",
    "#obtain accuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the emoticon look up table file\n",
    "header = ['EmoticonSymbol','SentimentScore']\n",
    "emoticon_data = pd.read_csv('EmoticonLookupTable.txt', delimiter='\\t', encoding = 'ISO-8859-1',names=header)\n",
    "#Writing emoticons to a dictionary\n",
    "emoji_dict = emoticon_data.groupby('EmoticonSymbol')['SentimentScore'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting acronyms and slangs from html page and creating a dictionary\n",
    "resp = requests.get(\"http://www.netlingo.com/acronyms.php\")\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "slangdict= {}\n",
    "key=\"\"\n",
    "value=\"\"\n",
    "for div in soup.findAll('div', attrs={'class':'list_box3'}):\n",
    "  for li in div.findAll('li'):\n",
    "   for a in li.findAll('a'):\n",
    "       key =a.text\n",
    "   value = li.text.split(key)[1]\n",
    "   slangdict[key]=value\n",
    "    \n",
    "#Removing the \"-or-\" terms in the dictionary and retaining one acronym\n",
    "for key,value in slangdict.items():\n",
    "    if \"-or-\" in value:\n",
    "       removestring = re.findall('-or-(.*)',value)\n",
    "       removestring = ''.join(removestring)\n",
    "       newvalue = value.replace(removestring,'')\n",
    "       newvalue = newvalue.replace(\"-or-\",'')\n",
    "       slangdict[key] = newvalue\n",
    "    elif \"-or\" in value:\n",
    "       removestring = re.findall('-or(.*)',value)\n",
    "       removestring = ''.join(removestring)\n",
    "       newvalue = value.replace(removestring,'')\n",
    "       newvalue = newvalue.replace(\"-or\",'')\n",
    "       slangdict[key] = newvalue\n",
    "    \n",
    "key_to_be_replaced = []\n",
    "for keys in slangdict.keys():\n",
    "    if \" or \" in keys:\n",
    "        key_to_be_replaced.append(keys)\n",
    "\n",
    "for keys in key_to_be_replaced:\n",
    "    getkeys = keys.split(\"or\")\n",
    "    for x in getkeys:\n",
    "        x = x.strip()\n",
    "        slangdict[x]= slangdict[keys]\n",
    "    slangdict.pop(keys,None)   \n",
    "\n",
    "# store the dictionary\n",
    "file = open(\"Slangdictionary.txt\",'w',encoding='utf-8')\n",
    "file.write(str(slangdict))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the slang dictionary that is already created\n",
    "f = open(\"Slangdictionary.txt\",\"r\")\n",
    "res1=f.read()\n",
    "f.close()\n",
    "slangdict = ast.literal_eval(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training data\n",
    "header = ['label','comment','parent_comment']\n",
    "cleaneddata = pd.read_table('clean_data_train_balanced_final.csv',\n",
    "                    sep='|', \n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to get list of emojis in a comment\n",
    "def find_emoji(text):\n",
    "    return list(x for x in text.split() if x in emoji_dict.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to extract features\n",
    "def featureextraction(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features for each comment \n",
    "# Punctuation Features and presence of sarcastic symbol and sentiment based features\n",
    "def allfeatures(user_comment):\n",
    "    # Punctuation Features \n",
    "    if '!' or '.' or '?' in user_comment:\n",
    "        Numofexclaimations = user_comment.count('!')\n",
    "        Numofdots = user_comment.count('.')\n",
    "        Numofquestionmarks = user_comment.count('?')\n",
    "    else:\n",
    "        Numofexclaimations = 0\n",
    "        Numofdots = 0\n",
    "        Numofquestionmarks = 0\n",
    "    # Presence of sarcastic symbol\n",
    "    if '(!)' in user_comment:\n",
    "        SarcasticSymbol = 1\n",
    "    else:\n",
    "        SarcasticSymbol = 0\n",
    "    \n",
    "    sentiments = TextBlob(str(user_comment)).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    subjectivity = sentiments.subjectivity\n",
    "    numofcapitals = sum(x.isupper() for x in user_comment.split() if len(x) > 1 )\n",
    "    elist = find_emoji(user_comment)\n",
    "    pscore =0\n",
    "    nscore = 0\n",
    "    for item in elist:\n",
    "        if (emoji_dict[item][0] == 1):\n",
    "            pscore += 1\n",
    "        elif (emoji_dict[item][0] == -1):\n",
    "            nscore += 1\n",
    "    return Numofexclaimations,Numofdots,Numofquestionmarks,SarcasticSymbol,polarity,subjectivity,numofcapitals,pscore,nscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper function to collect number of interjection\n",
    "def comment_interjection(user_comment):\n",
    "    count = Counter(tag for word,tag in user_comment)\n",
    "    return count['UH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  954.1302464008331\n",
      "time taken  566.874009847641\n"
     ]
    }
   ],
   "source": [
    "#feature set 1\n",
    "start_time = time.time() \n",
    "featureddataset = featureextraction(cleaneddata, 'comment', allfeatures, ['Numofexclaimations', 'Numofdots','Numofquestionmarks','SarcasticSymbol','Polarity', 'Subjectivity','NumofCapitalWords','PositiveEmojiCount','NegativeEmojiCount'])\n",
    "end_time = time.time() \n",
    "print(\"time taken \", end_time-start_time)\n",
    "\n",
    "\n",
    "feature extraction using POS\n",
    "start_time = time.time()\n",
    "txt = cleaneddata['comment'].tolist()\n",
    "POS tagging for all the tokens in the sentence\n",
    "tagged_texts = pos_tag_sents(map(word_tokenize, txt))\n",
    "end_time = time.time()\n",
    "cleaneddata['POS'] = tagged_texts\n",
    "print(\"time taken \", end_time-start_time)\n",
    "\n",
    "# number of interjection\n",
    "featureddataset['interjection']  = cleaneddata.POS.apply(comment_interjection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get parent comment\n",
    "def get_parent_sentiment(comment):\n",
    "    sentiments = TextBlob(str(comment)).sentiment\n",
    "    polarity = sentiments.polarity\n",
    "    if polarity >= 0.1:\n",
    "        return 1\n",
    "    elif polarity < -0.1:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "sia = SIA()\n",
    "#get heighly emotional words (associated with POS tags)\n",
    "def get_high_emotion_words(postags):\n",
    "    highly_pos = 0\n",
    "    highly_neg = 0\n",
    "    POS_list = ['JJ','JJR','JJS', 'RB','RBR','RBS','VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    num_tokens = len(postags)\n",
    "    for i in range(num_tokens):\n",
    "        if postags[i][1] in POS_list:            \n",
    "            #check sentiment of next word\n",
    "            if i < (num_tokens - 1) :\n",
    "                senti_word = sia.polarity_scores(postags[i+1][0])\n",
    "                if senti_word['pos'] == 1:\n",
    "                    highly_pos += 1\n",
    "                if senti_word['neg'] == 1:\n",
    "                    highly_neg += 1\n",
    "                \n",
    "    return highly_pos, highly_neg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional features pos_words, neg_words, flip_count\n",
    "def get_pos_neg_word_count(tokens):\n",
    "    \n",
    "    pos_word_count = 0\n",
    "    neg_word_count = 0    \n",
    "    pos_flag = False\n",
    "    neg_flag = False\n",
    "    flip_count = 0\n",
    "        \n",
    "    for word in tokens:\n",
    "        senti = sia.polarity_scores(str(word)) \n",
    "        if senti[\"pos\"] == 1:\n",
    "            pos_word_count += 1\n",
    "            pos_flag = True\n",
    "            if neg_flag:\n",
    "                flip_count += 1\n",
    "                neg_flag =  False\n",
    "                \n",
    "        elif senti[\"neg\"] == 1:\n",
    "            neg_word_count += 1\n",
    "            neg_flag = True\n",
    "            if pos_flag:\n",
    "                flip_count +=1\n",
    "                pos_flag = False\n",
    "    return pos_word_count, neg_word_count,flip_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  487.71743178367615\n",
      "time taken  931.3737943172455\n"
     ]
    }
   ],
   "source": [
    "# feature set 2\n",
    "start_time = time.time()\n",
    "#parent comment sentiment\n",
    "featureddataset['parent_sentiment'] = cleaneddata.parent_comment.apply(get_parent_sentiment)\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n",
    "#sentiment intensifier\n",
    "featureddataset['highly_positive'],featureddataset['highly_negative'] = zip(*cleaneddata['POS'].map(get_high_emotion_words))\n",
    "# sentiment word count\n",
    "start_time = time.time()\n",
    "emotion_dataset = featureextraction(featureddataset, 'comment', get_pos_neg_word_count, ['PosWords','NegWords','FlipCount'])\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#glove embedding\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# \n",
    "# load the Glove embedding into memory\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# Tokenize the comments\n",
    "Word_tokenizer = Tokenizer()\n",
    "Word_tokenizer.fit_on_texts(featureddataset['comment'])\n",
    "# Word_tokenizer.num_words = 100000\n",
    "vocab_size = len(Word_tokenizer.word_index) + 1\n",
    "#encode the train tokens to sequence\n",
    "sequences = Word_tokenizer.texts_to_sequences(featureddataset['comment'])\n",
    "\n",
    "# create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in Word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add embeddings to the comments\n",
    "emotion_dataset['embedding'] = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#helper to calculate the squareroot of square of the embedding matrix\n",
    "embedding_square ={}\n",
    "keys = range(vocab_size)\n",
    "for i in keys:\n",
    "    embedding = embedding_matrix[i]\n",
    "    sum_square = 0\n",
    "    for j in range(len(embedding)):\n",
    "        values = embedding[j]\n",
    "        sum_square += values*values\n",
    "    embedding_square[i] = math.sqrt(sum_square)\n",
    "    \n",
    "file = open(\"embedding_square_train.txt\",'w',encoding='utf-8')\n",
    "file.write(str(embedding_square))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# helper to calculate the cosine similarity between two words\n",
    "def cosine_similarity(word1,word2,v1,v2):\n",
    "#     \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxy = 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxy += x*y\n",
    "    return sumxy/(embedding_square[word1]*embedding_square[word2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to calculate the cosine similarity between the words in te comments\n",
    "def calculate_similarity(comment_token):\n",
    "    token_array = np.matrix(comment_token)\n",
    "    comment_len = token_array.shape[1]\n",
    "    most_similar = least_similar = most_dissimilar = least_dissimilar = 0\n",
    "    if comment_len > 0:\n",
    "        mat = np.empty(shape=(comment_len,comment_len))\n",
    "        mat[:] = np.nan\n",
    "        for i in range(0,comment_len):\n",
    "            for j in range(i+1,comment_len):\n",
    "                a = comment_token[i]\n",
    "                b = comment_token[j]\n",
    "                mat[i][j] = cosine_similarity(a,b,embedding_matrix[a],embedding_matrix[b].T)\n",
    "                mat[j][i] = mat[i][j]\n",
    "    #get the most similar\n",
    "        similar_mat = np.nanmax(mat,axis=0)\n",
    "        most_similar = np.nanmax(similar_mat)\n",
    "        least_similar = np.nanmin(similar_mat)\n",
    "    #get the most dissimilar \n",
    "        dissimilar_mat = np.nanmin(mat,axis=0)\n",
    "        most_dissimilar = np.nanmax(dissimilar_mat)\n",
    "        least_dissimilar = np.nanmin(dissimilar_mat)\n",
    "        \n",
    "    return most_similar, least_similar, most_dissimilar, least_dissimilar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: All-NaN slice encountered\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: All-NaN slice encountered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  5919.951149225235\n"
     ]
    }
   ],
   "source": [
    "# feature set 3\n",
    "start_time = time.time()\n",
    "embedded_dataset = featureextraction(emotion_dataset, 'embedding', calculate_similarity, ['most_similar','least_similar','most_dissimilar','least_dissimilar'])\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {'most_similar': 0, 'least_similar': 0, 'most_dissimilar': 0, 'least_dissimilar': 0}\n",
    "embedded_dataset = embedded_dataset.fillna(value=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create embedding features with word2vec for training data\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def allembeddingfeatures(user_comment):\n",
    "    #user_comment = user_comment.replace(\"'\",\"\")\n",
    "    result = text_to_word_sequence(user_comment,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True, split=' ')\n",
    "    #print(result)\n",
    "    length = len(result)\n",
    "    similaritymatrix = np.zeros((length,length))\n",
    "    i=0\n",
    "    for word in result:\n",
    "        wordseq = embeddings_dict[word]\n",
    "        j=0\n",
    "        # Similarity Matrix computation\n",
    "        for word1 in result:\n",
    "            if similaritymatrix[i][j] == 0:\n",
    "                wordseq1 = embeddings_dict[word1]\n",
    "                #print(wordseq.shape)\n",
    "                #print(wordseq1.shape)\n",
    "                k = cosine_similarity(wordseq,wordseq1)\n",
    "                similaritymatrix[i][j] = k\n",
    "                similaritymatrix[j][i] = k\n",
    "                j = j+1\n",
    "            else:\n",
    "                j = j+1\n",
    "        i = i + 1\n",
    "    if similaritymatrix.shape==(0,0):\n",
    "        maxmostsimilar = 0\n",
    "        minmostsimilar = 0\n",
    "        maxmostdissimilar = 0\n",
    "        minmostdissimilar = 0\n",
    "    else:    \n",
    "        # Comuting four features\n",
    "        np.fill_diagonal(similaritymatrix,0)\n",
    "        feature1 = similaritymatrix.max(axis=0)\n",
    "        maxmostsimilar = np.max(feature1)\n",
    "        minmostsimilar = np.min(feature1)\n",
    "        np.fill_diagonal(similaritymatrix,1)\n",
    "        feature2 = similaritymatrix.min(axis=0)\n",
    "        maxmostdissimilar = np.max(feature2)\n",
    "        minmostdissimilar = np.min(feature2)\n",
    "    return maxmostsimilar,minmostsimilar,maxmostdissimilar,minmostdissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create embedding features with word2vec for test data\n",
    "def alltestembeddingfeatures(user_comment):\n",
    "    #user_comment = user_comment.replace(\"'\",\"\")\n",
    "    result = text_to_word_sequence(user_comment,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True, split=' ')\n",
    "    #print(result)\n",
    "    length = len(result)\n",
    "    similaritymatrix = np.zeros((length,length))\n",
    "    i=0\n",
    "    for word in result:\n",
    "        wordseq = embeddings_test_dict[word]\n",
    "        j=0\n",
    "        # Similarity Matrix Computation\n",
    "        for word1 in result:\n",
    "            if similaritymatrix[i][j] == 0:\n",
    "                wordseq1 = embeddings_test_dict[word1]\n",
    "                #print(wordseq.shape)\n",
    "                #print(wordseq1.shape)\n",
    "                k = cosine_similarity(wordseq,wordseq1)\n",
    "                similaritymatrix[i][j] = k\n",
    "                similaritymatrix[j][i] = k\n",
    "                j = j+1\n",
    "            else:\n",
    "                j = j+1\n",
    "        i = i + 1\n",
    "    if similaritymatrix.shape==(0,0):\n",
    "        maxmostsimilar = 0\n",
    "        minmostsimilar = 0\n",
    "        maxmostdissimilar = 0\n",
    "        minmostdissimilar = 0\n",
    "    else:    \n",
    "        #Computing four features\n",
    "        np.fill_diagonal(similaritymatrix,0)\n",
    "        feature1 = similaritymatrix.max(axis=0)\n",
    "        maxmostsimilar = np.max(feature1)\n",
    "        minmostsimilar = np.min(feature1)\n",
    "        np.fill_diagonal(similaritymatrix,1)\n",
    "        feature2 = similaritymatrix.min(axis=0)\n",
    "        maxmostdissimilar = np.max(feature2)\n",
    "        minmostdissimilar = np.min(feature2)\n",
    "    return maxmostsimilar,minmostsimilar,maxmostdissimilar,minmostdissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the comments to sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "def comment_clean1(user_comment):  \n",
    "    result = text_to_word_sequence(user_comment,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True, split=' ')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the word2vec model created\n",
    "traindata1=featureddataset\n",
    "traindata1['comment'] = traindata1.comment.apply(comment_clean1)\n",
    "usercomment1 = traindata1['comment'].values.tolist()\n",
    "embedding_dim = 100\n",
    "model = Word2Vec(usercomment1, size=embedding_dim, window=5,workers=4, min_count=1)\n",
    "words = list(model.wv.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the model created to a file with train data embeddings\n",
    "file = \"word2vec_embedding_train_data.txt\"\n",
    "model.wv.save_word2vec_format(file,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the embedding dictionary with words from training data\n",
    "import os\n",
    "embeddings_dict = {}\n",
    "f = open(\"word2vec_embedding_train_data.txt\", encoding = \"utf-8\")\n",
    "i=1\n",
    "for line in f:\n",
    "        line = line.replace('\\U00002013', '-')\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        value = ' '.join(values[1:])\n",
    "        val = list(map(float, value.split()))\n",
    "        coefs = np.array([val])\n",
    "        #print(coefs.shape)\n",
    "        coefs = [np.asarray(val)]\n",
    "        #print(coefs)\n",
    "        embeddings_dict[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the word2vec embeddings with existing features\n",
    "embedded_dataset = featureextraction(emotion_dataset, 'comment', allembeddingfeatures, ['Maxmostsimilar','Minmostsimilar','Maxmostdissimilar','Minmostdissimilar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the cleaned data with features into a csv file\n",
    "embedded_dataset.to_csv('clean_data_with_all_features.csv',\n",
    "           sep= '|',\n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps for test data preparation\n",
    "# read the test data\n",
    "header = ['label','comment','parent_comment']\n",
    "testdata = pd.read_table('clean_data_test_balanced_Wparent.csv',\n",
    "                    sep='|', \n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  676.2500035762787\n",
      "time taken  143.84018182754517\n"
     ]
    }
   ],
   "source": [
    "#feature set 1\n",
    "start_time = time.time() \n",
    "featuredset = featureextraction(testdata, 'comment', allfeatures, ['Numofexclaimations', 'Numofdots','Numofquestionmarks','SarcasticSymbol','Polarity', 'Subjectivity','NumofCapitalWords','PositiveEmojiCount','NegativeEmojiCount'])\n",
    "end_time = time.time() \n",
    "print(\"time taken \", end_time-start_time)\n",
    "#feature extraction using POS\n",
    "start_time = time.time()\n",
    "txt = testdata['comment'].tolist()\n",
    "#POS tagging for all the tokens in the sentence\n",
    "tagged_texts = pos_tag_sents(map(word_tokenize, txt))\n",
    "end_time = time.time()\n",
    "testdata['POS'] = tagged_texts\n",
    "print(\"time taken \", end_time-start_time)\n",
    "\n",
    "# number of interjection\n",
    "featuredset['interjection']  = testdata.POS.apply(comment_interjection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  122.3596031665802\n",
      "time taken  198.8806071281433\n"
     ]
    }
   ],
   "source": [
    "# feature set 2\n",
    "start_time = time.time()\n",
    "#parent comment sentiment\n",
    "featuredset['parent_sentiment'] = testdata.parent_comment.apply(get_parent_sentiment)\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n",
    "#sentiment intensifier\n",
    "featuredset['highly_positive'],featuredset['highly_negative'] = zip(*testdata['POS'].map(get_high_emotion_words))\n",
    "# sentiment word count\n",
    "start_time = time.time()\n",
    "test_dataset = featureextraction(featuredset, 'comment', get_pos_neg_word_count, ['PosWords','NegWords','FlipCount'])\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the comments\n",
    "\n",
    "Word_tokenizer.fit_on_texts(featuredset['comment'])\n",
    "# Word_tokenizer.num_words = 100000\n",
    "vocab_size = len(Word_tokenizer.word_index) + 1\n",
    "#encode the train tokens to sequence\n",
    "sequences = Word_tokenizer.texts_to_sequences(featuredset['comment'])\n",
    "\n",
    "# create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in Word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add embeddings to the comments\n",
    "test_dataset['embedding'] = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper to calculate the squareroot of square of the embedding matrix\n",
    "embedding_square ={}\n",
    "keys = range(vocab_size)\n",
    "for i in keys:\n",
    "    embedding = embedding_matrix[i]\n",
    "    sum_square = 0\n",
    "    for j in range(len(embedding)):\n",
    "        values = embedding[j]\n",
    "        sum_square += values*values\n",
    "    embedding_square[i] = math.sqrt(sum_square)\n",
    "    \n",
    "file = open(\"embedding_square_test.txt\",'w',encoding='utf-8')\n",
    "file.write(str(embedding_square))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: All-NaN slice encountered\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: All-NaN slice encountered\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: All-NaN slice encountered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  1327.6528935432434\n"
     ]
    }
   ],
   "source": [
    "# feature set 3\n",
    "start_time = time.time()\n",
    "embedded_testset = featureextraction(test_dataset, 'embedding', calculate_similarity, ['most_similar','least_similar','most_dissimilar','least_dissimilar'])\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {'most_similar': 0, 'least_similar': 0, 'most_dissimilar': 0, 'least_dissimilar': 0}\n",
    "embedded_testset = embedded_testset.fillna(value=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the word2vec model for test data\n",
    "testdata1 = test_dataset\n",
    "testdata1['comment'] = testdata1.comment.apply(comment_clean1)\n",
    "usercomment10 = testdata1['comment'].values.tolist()\n",
    "model = Word2Vec(usercomment10, size=embedding_dim, window=5,workers=4, min_count=1)\n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to a file\n",
    "file = \"word2vec_embedding_test_data.txt\"\n",
    "model.wv.save_word2vec_format(file,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embeddings dictionary for test data\n",
    "embeddings_test_dict = {}\n",
    "f = open(\"word2vec_embedding_test_data.txt\", encoding = \"utf-8\")\n",
    "i=1\n",
    "for line in f:\n",
    "        line = line.replace('\\U00002013', '-')\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        value = ' '.join(values[1:])\n",
    "        val = list(map(float, value.split()))\n",
    "        coefs = np.array([val])\n",
    "        #print(coefs.shape)\n",
    "        coefs = [np.asarray(val)]\n",
    "        #print(coefs)\n",
    "        embeddings_test_dict[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the four embedding features of word2vec\n",
    "embedded_testset = featureextraction(test_dataset, 'comment', alltestembeddingfeatures, ['Maxmostsimilar','Minmostsimilar','Maxmostdissimilar','Minmostdissimilar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the cleaned data with features into a csv file\n",
    "embedded_testset.to_csv('clean_testdata_with_all_features.csv',\n",
    "           sep= '|',\n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train data with features\n",
    "traindata_withfeature = pd.read_table('clean_data_with_all_features.csv',\n",
    "                    sep='|', \n",
    "                   # delimiter=',',\n",
    "#                     usecols=[0,1,2],\n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)\n",
    "\n",
    "# load the test data with features\n",
    "testdata_withfeature = pd.read_table('clean_testdata_with_all_features.csv',\n",
    "                    sep='|', \n",
    "                   # delimiter=',',\n",
    "#                     usecols=[0,1,2],\n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop embedding\n",
    "traindata_withfeature = traindata_withfeature.drop(columns=['embedding'])\n",
    "\n",
    "testdata_withfeature = testdata_withfeature.drop(columns=['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop parent sentiment\n",
    "# parent_sentiment\n",
    "traindata_Wparent = traindata_withfeature\n",
    "traindata_Wparent = traindata_Wparent.drop(columns=['parent_sentiment'])\n",
    "\n",
    "testdata_Wparent = testdata_withfeature\n",
    "testdata_Wparent = testdata_Wparent.drop(columns=['parent_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop comments with comment length < 2\n",
    "traindata = traindata_withfeature\n",
    "# droped_train = traindata.where(traindata['comment'].str.split().str.len()>1)\n",
    "droped_train = traindata[traindata['comment'].str.split().str.len()>2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226169, 23)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata = testdata_withfeature\n",
    "droped_test = testdata[testdata['comment'].str.split().str.len()>2]\n",
    "droped_test.shape\n",
    "# testdata_withfeature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop new added features(5)\n",
    "traindata_parent = traindata_withfeature\n",
    "traindata_parent = traindata_parent.drop(columns=['highly_positive','highly_negative','PosWords','NegWords','FlipCount'])\n",
    "\n",
    "testdata_parent = testdata_withfeature\n",
    "testdata_parent = testdata_parent.drop(columns=['highly_positive','highly_negative','PosWords','NegWords','FlipCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with only embeddings\n",
    "traindata = traindata_withfeature\n",
    "traindata = traindata.drop(columns=['parent_sentiment','highly_positive','highly_negative','PosWords','NegWords','FlipCount'])\n",
    "\n",
    "testdata = testdata_withfeature\n",
    "testdata = testdata.drop(columns=['parent_sentiment','highly_positive','highly_negative','PosWords','NegWords','FlipCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data features\n",
    "newtrain = pd.DataFrame(droped_train.iloc[:, 3:])\n",
    "#train data labels\n",
    "targetlabel = droped_train.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data features\n",
    "newtest = pd.DataFrame(droped_test.iloc[:, 3:])\n",
    "#test data labels\n",
    "testlabel = droped_test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  53.68022584915161\n"
     ]
    }
   ],
   "source": [
    "# gradient boosting algorithm\n",
    "start_time = time.time()\n",
    "gradient_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5, max_depth=1)\n",
    "gradient_clf.fit(newtrain,targetlabel)\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[69167 39977]\n",
      " [50156 66869]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.63      0.61    109144\n",
      "           1       0.63      0.57      0.60    117025\n",
      "\n",
      "   micro avg       0.60      0.60      0.60    226169\n",
      "   macro avg       0.60      0.60      0.60    226169\n",
      "weighted avg       0.60      0.60      0.60    226169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gradient_predictions = gradient_clf.predict(newtest)\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(testlabel, gradient_predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(testlabel, gradient_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken  23.603333711624146\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "start_time = time.time()\n",
    "random_clf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "random_clf.fit(newtrain,targetlabel)\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[75569 39622]\n",
      " [57933 63068]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.66      0.61    115191\n",
      "           1       0.61      0.52      0.56    121001\n",
      "\n",
      "   micro avg       0.59      0.59      0.59    236192\n",
      "   macro avg       0.59      0.59      0.59    236192\n",
      "weighted avg       0.59      0.59      0.59    236192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random_predictions = random_clf.predict(newtest)\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(testlabel, random_predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(testlabel, random_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC  \n",
    "start_time = time.time()\n",
    "# linear kernel\n",
    "svclassifier = SVC(kernel='linear',C=1,gamma=1)  \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "\n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Guassian kernel\n",
    "start_time = time.time()\n",
    "svclassifier = SVC(kernel='rbf',C=1,gamma=1)  \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# polynomial kernel with degree 3\n",
    "start_time = time.time()\n",
    "svclassifier = SVC(kernel='poly', degree=3) \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = svclassifier.predict(X_test) \n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
