{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Author : Karuna\n",
    "# Different CNN models tried "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the test data from the csv file\n",
    "testdata = pd.read_table('clean_data_test_balanced.csv',\n",
    "                    sep='|',\n",
    "                    usecols=[0,1],\n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the training data from the csv file\n",
    "traindata = pd.read_table('clean_data_train_balanced.csv',\n",
    "                    sep='|', \n",
    "                    usecols=[0,1],\n",
    "                    dtype={'label':int,'comment':str},\n",
    "                    keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data is  (243572, 2)\n",
      "Train data is  (978039, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test data is \", testdata.shape)\n",
    "print(\"Train data is \", traindata.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data, test_data = train_test_split(training_data, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing datasets into train and test\n",
    "X_train = traindata['comment']\n",
    "y_train = traindata['label']\n",
    "X_test = testdata['comment']\n",
    "y_test = testdata['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(978039,)\n",
      "(243572,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwordlist(text):\n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()    \n",
    "    text = \" \".join(text)    \n",
    "    #Remove Special Characters\n",
    "    text=re.sub(r'[^a-z\\d ]','',text)    \n",
    "    #Replace Numbers\n",
    "    text=re.sub(r'\\d+','n',text)\n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []\n",
    "for text in X_train:\n",
    "    comments.append(getwordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments=[]\n",
    "for text in X_test:\n",
    "    test_comments.append(getwordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words 207219\n",
      "Train data shape: (978039, 2500)\n",
      "Test data shape: (243572, 2500)\n"
     ]
    }
   ],
   "source": [
    "# Use Kears tokenizer to get word vectors\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(comments + test_comments) \n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('unique words %s' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(sequences, maxlen=2500, padding='post')\n",
    "print('Train data shape:', train_data.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=2500, padding='post')\n",
    "print('Test data shape:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Glove Vectors loaded\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embeddings for layer\n",
    "words_max = len(word_index)+1\n",
    "embedding_matrix = np.zeros((len(word_index)+1, 100))\n",
    "for word, i in word_index.items():\n",
    "    if i >= words_max:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import statements\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense ,Dropout, Input, Activation\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, Conv1D, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an non re-trainable embedding layer with embedding size 100\n",
    "nb_words = len(word_index)+1\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        100,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=2500,\n",
    "        trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2000, 100)         18109000  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 200001    \n",
      "=================================================================\n",
      "Total params: 18,309,001\n",
      "Trainable params: 200,001\n",
      "Non-trainable params: 18,109,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 1st tried model \n",
    "#Simple NN with embedding layer and final dense with relu activation\n",
    "#binary_crossentropy used as loss function and RMSprop as optimizer\n",
    "# Model gave accuracy of 59.6%\n",
    "model_1 = Sequential()\n",
    "model_1.add(embedding_layer)\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(1, activation='relu'))\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "    optimizer='RMSprop',\n",
    "    metrics=['accuracy'])\n",
    "print(model_1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782431/782431 [==============================] - 648s 828us/step - loss: 1.0084 - acc: 0.5843s -\n",
      "Epoch 2/2\n",
      "782431/782431 [==============================] - 734s 938us/step - loss: 1.0407 - acc: 0.5870\n"
     ]
    }
   ],
   "source": [
    "hist = model_1.fit(train_data, y_train, \n",
    "        epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195608/195608 [==============================] - 167s 855us/step\n",
      "Accuracy: 59.609014\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_1.evaluate(test_data, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2000, 100)         18109000  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 200001    \n",
      "=================================================================\n",
      "Total params: 18,309,001\n",
      "Trainable params: 200,001\n",
      "Non-trainable params: 18,109,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# secon dmodel, simple NN with output dense but sigmoid as activation\n",
    "#optimizer is changed to adam, gave accuracy of 63.04%\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782431/782431 [==============================] - 678s 866us/step - loss: 0.6444 - acc: 0.6235\n",
      "Epoch 2/2\n",
      "782431/782431 [==============================] - 667s 852us/step - loss: 0.6393 - acc: 0.6309\n"
     ]
    }
   ],
   "source": [
    "hist_1 = model.fit(train_data, y_train,epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195608/195608 [==============================] - 157s 801us/step\n",
      "Accuracy: 63.049569\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_data, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2000, 100)         18109000  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                3200016   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 21,309,033\n",
      "Trainable params: 3,200,033\n",
      "Non-trainable params: 18,109,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "782431/782431 [==============================] - 3134s 4ms/step - loss: 0.6482 - acc: 0.6149\n",
      "Epoch 2/5\n",
      "782431/782431 [==============================] - 3115s 4ms/step - loss: 0.6361 - acc: 0.6281\n",
      "Epoch 3/5\n",
      "782431/782431 [==============================] - 3109s 4ms/step - loss: 0.6317 - acc: 0.6325\n",
      "Epoch 4/5\n",
      "782431/782431 [==============================] - 3108s 4ms/step - loss: 0.6290 - acc: 0.6354\n",
      "Epoch 5/5\n",
      "782431/782431 [==============================] - 3115s 4ms/step - loss: 0.6268 - acc: 0.6384\n",
      "195608/195608 [==============================] - 146s 749us/step\n",
      "Accuracy: 65.221770\n"
     ]
    }
   ],
   "source": [
    "# third try, NN model with embedding layer, global max pooling and dense with 16 units having activation as Relu\n",
    "#dropout layer has been added to reduce overfitting\n",
    "# model gave accuracy of 65%\n",
    "model_2 = Sequential()\n",
    "model_2.add(embedding_layer)\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(GlobalMaxPooling1D())\n",
    "model_2.add(Dense(16, activation='relu'))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model_2.summary())\n",
    "\n",
    "hist_2 = model_2.fit(train_data, y_train,epochs=5, batch_size=512)\n",
    "\n",
    "loss, accuracy = model_2.evaluate(test_data, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2000, 100)         18109000  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 200001    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 18,309,005\n",
      "Trainable params: 200,005\n",
      "Non-trainable params: 18,109,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "782431/782431 [==============================] - 2878s 4ms/step - loss: 0.4951 - acc: 0.5049\n",
      "Epoch 2/10\n",
      "782431/782431 [==============================] - 2691s 3ms/step - loss: 0.4951 - acc: 0.5049\n",
      "Epoch 3/10\n",
      "782431/782431 [==============================] - 2647s 3ms/step - loss: 0.4951 - acc: 0.5049\n",
      "Epoch 4/10\n",
      "782431/782431 [==============================] - 2681s 3ms/step - loss: 0.4951 - acc: 0.5049\n",
      "Epoch 5/10\n",
      "782431/782431 [==============================] - 2556s 3ms/step - loss: 0.4951 - acc: 0.5049\n",
      "Epoch 6/10\n",
      "782431/782431 [==============================] - 2602s 3ms/step - loss: 0.4951 - acc: 0.5049\n",
      "Epoch 7/10\n",
      "782431/782431 [==============================] - 2586s 3ms/step - loss: 0.4951 - acc: 0.5049\n",
      "Epoch 8/10\n",
      "782431/782431 [==============================] - 2614s 3ms/step - loss: 0.4951 - acc: 0.5049\n",
      "Epoch 9/10\n",
      "782431/782431 [==============================] - 2635s 3ms/step - loss: 0.4951 - acc: 0.5049\n",
      "Epoch 10/10\n",
      "782431/782431 [==============================] - 2604s 3ms/step - loss: 0.4951 - acc: 0.5049\n",
      " 49600/195608 [======>.......................] - ETA: 1:10"
     ]
    }
   ],
   "source": [
    "#added a dense layer with relu and final layer with softmax activation\n",
    "# stopped running it due to low accuracy\n",
    "\n",
    "model_3 = Sequential()\n",
    "model_3.add(embedding_layer)\n",
    "model_3.add(Flatten())\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(1, activation='relu'))\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "model_3.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model_3.compile(optimizer='adam', loss='mse', metrics=['acc'])\n",
    "\n",
    "print(model_3.summary())\n",
    "\n",
    "hist_3 = model_3.fit(train_data, y_train,epochs=10, batch_size=512)\n",
    "\n",
    "loss, accuracy = model_3.evaluate(test_data, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 2500, 100)         20722000  \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 2498, 250)         75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                4016      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,801,283\n",
      "Trainable params: 79,283\n",
      "Non-trainable params: 20,722,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 831333 samples, validate on 146706 samples\n",
      "Epoch 1/12\n",
      "831333/831333 [==============================] - 5650s 7ms/step - loss: 0.6243 - acc: 0.6471 - val_loss: 0.5822 - val_acc: 0.6905\n",
      "Epoch 2/12\n",
      "831333/831333 [==============================] - 5636s 7ms/step - loss: 0.5893 - acc: 0.6837 - val_loss: 0.5756 - val_acc: 0.6952\n",
      "Epoch 3/12\n",
      "831333/831333 [==============================] - 5594s 7ms/step - loss: 0.5773 - acc: 0.6943 - val_loss: 0.5596 - val_acc: 0.7060\n",
      "Epoch 4/12\n",
      "831333/831333 [==============================] - 5956s 7ms/step - loss: 0.5689 - acc: 0.7023 - val_loss: 0.5657 - val_acc: 0.7031\n",
      "Epoch 5/12\n",
      "831333/831333 [==============================] - 5563s 7ms/step - loss: 0.5631 - acc: 0.7077 - val_loss: 0.5525 - val_acc: 0.7139\n",
      "Epoch 6/12\n",
      "831333/831333 [==============================] - 5348s 6ms/step - loss: 0.5583 - acc: 0.7120 - val_loss: 0.5488 - val_acc: 0.7152\n",
      "Epoch 7/12\n",
      "831333/831333 [==============================] - 5335s 6ms/step - loss: 0.5548 - acc: 0.7148 - val_loss: 0.5629 - val_acc: 0.7068\n",
      "Epoch 8/12\n",
      "831333/831333 [==============================] - 5346s 6ms/step - loss: 0.5514 - acc: 0.7177 - val_loss: 0.5478 - val_acc: 0.7171\n",
      "Epoch 9/12\n",
      "831333/831333 [==============================] - 5345s 6ms/step - loss: 0.5481 - acc: 0.7207 - val_loss: 0.5459 - val_acc: 0.7181\n",
      "Epoch 10/12\n",
      "831333/831333 [==============================] - 5673s 7ms/step - loss: 0.5452 - acc: 0.7231 - val_loss: 0.5582 - val_acc: 0.7097\n",
      "Epoch 11/12\n",
      "831333/831333 [==============================] - 5611s 7ms/step - loss: 0.5441 - acc: 0.7239 - val_loss: 0.5520 - val_acc: 0.7154\n",
      "Epoch 12/12\n",
      "831333/831333 [==============================] - 5579s 7ms/step - loss: 0.5411 - acc: 0.7264 - val_loss: 0.5470 - val_acc: 0.7181\n",
      "243572/243572 [==============================] - 331s 1ms/step\n",
      "Accuracy: 71.183880\n"
     ]
    }
   ],
   "source": [
    "# CNN model 1\n",
    "# 1D convolutional layer with 250 units and kernal size 3. added dense lauer of 16 with activation as tanh.\n",
    "# 2 dropuout layers are added to reduce overfitting, final layer is with sigmoid activation\n",
    "# gave accuracy of 71.8%\n",
    "cb = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, mode='auto')\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(embedding_layer)\n",
    "\n",
    "model_4.add(Conv1D(250, 3, padding='valid',activation='relu',strides=1))\n",
    "model_4.add(GlobalMaxPooling1D())\n",
    "model_4.add(Dropout(0.2))\n",
    "model_4.add(Dense(16, activation='tanh'))\n",
    "model_4.add(Dropout(0.2))\n",
    "model_4.add(Dense(1, activation='sigmoid'))\n",
    "model_4.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(model_4.summary())\n",
    "\n",
    "hist_4 = model_4.fit(train_data, y_train,epochs=12, batch_size=512, callbacks=[cb], validation_split=0.15)\n",
    "\n",
    "loss, accuracy = model_4.evaluate(test_data, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243572/243572 [==============================] - 452s 2ms/step\n",
      "Accuracy: 71.183880\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_4.evaluate(test_data, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Savinf the model to the disk\n",
    "from keras.models import model_from_json \n",
    "# serialize model to JSON\n",
    "model_json = model_4.to_json()\n",
    "with open(\"model_4.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model_4.save_weights(\"model_4.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "243572/243572 [==============================] - 333s 1ms/step\n",
      "acc: 71.18%\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model_4.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_4.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(test_data, y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90728 29836]\n",
      " [40352 82656]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72    120564\n",
      "           1       0.73      0.67      0.70    123008\n",
      "\n",
      "   micro avg       0.71      0.71      0.71    243572\n",
      "   macro avg       0.71      0.71      0.71    243572\n",
      "weighted avg       0.71      0.71      0.71    243572\n",
      "\n",
      "0.7118387992051631\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_predict))  \n",
    "print(classification_report(y_test,y_predict))  \n",
    "print(accuracy_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 2500, 100)         20722000  \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 2498, 250)         75250     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                4016      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 20,801,283\n",
      "Trainable params: 20,801,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 831333 samples, validate on 146706 samples\n",
      "Epoch 1/5\n",
      "831333/831333 [==============================] - 11005s 13ms/step - loss: 0.5930 - acc: 0.6798 - val_loss: 0.5428 - val_acc: 0.7223\n",
      "Epoch 2/5\n",
      "831333/831333 [==============================] - 11024s 13ms/step - loss: 0.5360 - acc: 0.7310 - val_loss: 0.5406 - val_acc: 0.7243\n",
      "Epoch 3/5\n",
      "831333/831333 [==============================] - 11063s 13ms/step - loss: 0.4951 - acc: 0.7610 - val_loss: 0.5484 - val_acc: 0.7225\n",
      "Epoch 4/5\n",
      "831333/831333 [==============================] - 9655s 12ms/step - loss: 0.4535 - acc: 0.7862 - val_loss: 0.5655 - val_acc: 0.7214\n",
      "Epoch 5/5\n",
      "831333/831333 [==============================] - 8767s 11ms/step - loss: 0.4188 - acc: 0.8062 - val_loss: 0.5900 - val_acc: 0.7157\n",
      "243572/243572 [==============================] - 510s 2ms/step\n",
      "Accuracy: 70.839013\n"
     ]
    }
   ],
   "source": [
    "# Model with re-trainable embeddings\n",
    "# for the CNN model with 71.18% accuracy, changed the embedding layer as re-trainable.\n",
    "# model overfitted the data while training with 80.62 accuracy, but reduced to 70.8% on test data\n",
    "nb_words = len(word_index)+1\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        100,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=2500,\n",
    "        trainable=True)\n",
    "\n",
    "cb = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, mode='auto')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(250, 3, padding='valid',activation='relu',strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "hist = model.fit(train_data, y_train,epochs=5, batch_size=512, callbacks=[cb], validation_split=0.15)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_data, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
